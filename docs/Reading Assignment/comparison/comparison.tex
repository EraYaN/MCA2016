% metric is SPEC95 branch prediction correctness (between 0-1) Quantitative
% metric is size on-chip Qualitative

\section{Comparison}
\subsection{Metrics}
As a comparison metric the branch prediction accuracy is the most important specification. This specification can be derived via the SPEC benchmark, which is displayed for every branch predictor in tables \ref{tab:spec-accuracy-big} and \ref{tab:spec-accuracy-small}. The accuracy of the difference predictors is shown, note that some of these values were retrieved from a figure so there is a margin of error. It should also be noted that SPEC benchmarks are updated over time and so some of the branch predictors are compared using different SPEC benchmarks.\\A second important metric we will discuss is the size of each of the solutions and their relative performance.
\subsection{Results}
\label{ssec:results}
\begin{table}[H]
    \centering
    \caption{SPEC benchmarks accuracy result for big predictor sizes.}
    \label{tab:spec-accuracy-big}
    \begin{tabular}{llS[table-format=3.1,table-space-text-post=\si{\kilo\byte}]S[table-format=1.3]}
    \toprule
            {\textbf{Predictor}} & {\textbf{Benchmarks}} & {\textbf{Size ($\approx$)}} & {\textbf{Accuracy}} \\
        \midrule
            {Combined (local/gshare)} & SPEC 89 & 64\si{\kilo\byte} & 0.981 \\
            {Two-level} & SPEC 89 & 14\si{\kilo\byte} & 0.97 \\
            {Standard Neural} & SPEC 95 + 00 (int) & 64\si{\kilo\byte} & 0.939 \\
            {Fast-path Neural} & SPEC 95 + 00 (int) & 64\si{\kilo\byte} & 0.942 \\
            {gshare} & SPEC95 & 38\si{\kilo\byte}  & 0.945 \\
            {bi-mode} & SPEC95 & 35\si{\kilo\byte}  & 0.950 \\
            {YAGS} & SPEC95 & 48\si{\kilo\byte}  & 0.955 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[H]
    \centering
    \caption{SPEC benchmarks accuracy result for small predictor sizes.}
    \label{tab:spec-accuracy-small}
    \begin{tabular}{llS[table-format=3.1,table-space-text-post=\si{\kilo\byte}]S[table-format=1.3]}
        \toprule
            {\textbf{Predictor}} & {\textbf{Benchmarks}} & {\textbf{Size ($\approx$)}} & {\textbf{Accuracy}} \\
        \midrule
            {Combined (local/gshare)} & SPEC 89 & 0.5\si{\kilo\byte} & 0.952 \\
            {Two-level} & N/A & & \\
            {Standard Neural} & SPEC 95 + 00 (int) & 1\si{\kilo\byte} & 0.912 \\
            {Fast-path Neural} & SPEC 95 + 00 (int) & 1\si{\kilo\byte} & 0.925 \\
            {gshare} & SPEC95 & 0.5\si{\kilo\byte} & 0.69 \\
            {bi-mode} & SPEC95 & 0.5\si{\kilo\byte} & 0.73 \\
            {YAGS} & SPEC95 & 0.5\si{\kilo\byte} & 0.77 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection*{Combined}
The combined branch predictor is benchmarked using SPEC 89, running the programs for 10 million instructions.
For large predictor sizes, around 64 KB, table \ref{tab:spec-accuracy-big} shows that the accuracy of the best performing predictor combination appears to be two-level/gshare as it approaches 98.1\%. This is better than either of the two alone. Decreasing the size of the predictor leads to less accuracy as shown in table \ref{tab:spec-accuracy-small}. However the combined predictor still has the best prediction accuracy. A great advantage of the combined predictor is that it's very adaptable. There are a lot of combinations of predictors that could possibly be implemented and if resources are at a premium, the predictor can easily be shrunk at the cost of performance.

\subsubsection*{Two-level}
On nine of the SPEC benchmarks two-level adaptive training manages to attain an average accuracy of 97\% with a 12 bit 512-entry 4-way associative history register table with a pattern table comprised of saturating counters. 
The size of the two-level predictor that is mentioned in the table is calculated as the addition of the size of the HRT (512 entries of 12 bits) and the PT ($2^{12}$ entries of 2 bits). 
The data in Yeh and Patt's paper doesn't lend itself to a good comparison based on predictor size which is why the two-level entry in the small predictor table is left blank \cite{twolevel}. 
Adjusting the amount of bits in the history registers and the amount of history registers in the HRT still do allow for a predictor that is very adjustable in size.


\subsubsection*{Standard Neural, Fast-path Neural}
The neural predictors are mainly simulated with the SPEC CPU 2000 integer benchmark next to some SPEC 95 integer benchmarks that are not duplicated in SPEC 2000. From table \ref{tab:spec-accuracy-big} it can be seen that the standard neural branch prediction and the fast-path neural branch prediction are not among the best. Looking at table \ref{tab:spec-accuracy-small} however, the predictors are now the second best options. In both cases is the fast-path based algorithm faster than the standard implementation. The parallelization of the algorithm proves to be beneficial. Decreasing the size has little influence on the prediction accuracy with about a difference of 2 percent point. Similarly however this means that increasing the size will not lead to much better accuracy. The fast-path based algorithm takes the path history into account but this influence levels off as the size increases.

\subsubsection*{gshare, bi-mode, YAGS}
gshare, bi-mode, and YAGS predictors were simulated using all 8 SPEC 95 benchmarks interleaved each 60 000 instructions to simulate a high context switch environment.
This is probably also why the accuracy of these algorithms is so poor compared to the other paper's figures.

YAGS performs much better when it is scaled down compared to the gshare and bi-mode predictors \cite{yags}.
This is because the YAGS prediction contains less useless information in it's tables, as described in subsection \ref{ssec:yags}.
So you can get the same performance with less resources.

Eden and Mudge believe that they have not shown the full potential of this predictor scheme.
Due to the relatively small size of the direction caches, the history register is also quite small, hence the predictor has reduced correlation information.
A proposed improvement is to add those lost history bits as tags.
This can be done for all the predictors in the paper, but they stated that the overhead would be much smaller for the YAGS prediction scheme \cite{yags}. 
