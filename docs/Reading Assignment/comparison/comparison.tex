% metric is SPEC95 branch prediction correctness (between 0-1) Quantitative
% metric is size on-chip Qualitative

\section{Comparison}
\subsection{Metrics}
As a comparison metric the branch prediction accuracy is the most important specification.
The accuracy of the different predictors is shown in tables \ref{tab:spec-accuracy-big} and \ref{tab:spec-accuracy-small}, note that some of these values were retrieved from a figure so there is a margin of error.
It should also be noted that SPEC benchmarks are updated over time and so some of the branch predictors are compared using different SPEC benchmarks.\\
A second important metric we will discuss is the size of each of the solutions and their relative performance.

To help visualize which of these solutions performs best some derived metrics are introduced.
Due to the different scale and impact of the two base metrics, we normalize and scale the numbers.
Equation \ref{eq:accuracy-norm} is used to calculate the Normalized Accuracy ($A_N$) from the Accuracy ($A$).
The accuracy needs to be normalized because the range is much smaller than a full \SIrange{0}{100}{\percent}and it should have a bigger effect of our performance metric.
The margin ($M$) used was \SI{5}{\percent}.
\begin{equation}
\label{eq:accuracy-norm}
A_{N} = \frac{A - (min([A]) - M)}{(max([A]) + M)-(min([A]) - M)}
\end{equation}

Equation \ref{eq:performance} is used to calculate the Performance ($P$).
Subsequently it's normalized as shown in Equation \ref{eq:performance-norm}.
The margin used was the difference between the minimum and maximum value of the performance column.
These margins are used to make sure that the values are closer to each other therefore the outliers don't distort the results too much.

\begin{equation}
\label{eq:performance}
P = \frac{A_{N}}{Size}
\end{equation}

\begin{equation}
\label{eq:performance-norm}
P_{N} = \frac{P - (min([P]) - M)}{(max([P]) + M)-(min([P]) - M)}
\end{equation}


\subsection{Results}
\label{ssec:results}
\begin{table*}[h]
    \centering
    \caption{SPEC benchmarks accuracy result for big predictor sizes.}
    \label{tab:spec-accuracy-big}
    \begin{tabular}{llS[table-format=3.1,table-space-text-post=\si{\kilo\byte}]S[table-format=1.3]S[table-format=1.3]S[table-format=1.3]}
    \toprule
            {\textbf{Predictor}} & {\textbf{Benchmarks}} & {\textbf{Size ($\approx$)}} & {\textbf{Accuracy}} & {\textbf{Accuracy (Normalized)}} & {\textbf{Performance (Normalized)}}\\
        \midrule
            {Combined (local/gshare)} & SPEC 89     & 38\si{\kilo\byte} & 0.981 & 0.657534247 & 0.666666667 \\
            {Two-level} & SPEC 89                   & 34\si{\kilo\byte} & 0.970 & 0.582191781 & 0.661453242 \\
            {Standard Neural} & SPEC 95 + 00 (int)  & 64\si{\kilo\byte} & 0.939 & 0.369863014 & 0.333333333 \\
            {Fast-path Neural} & SPEC 95 + 00 (int) & 64\si{\kilo\byte} & 0.942 & 0.390410959 & 0.342619746 \\
            {gshare} & SPEC 95                      & 38\si{\kilo\byte} & 0.935 & 0.342465753 & 0.426849137 \\
            {bi-mode} & SPEC 95                     & 35\si{\kilo\byte} & 0.940 & 0.376712329 & 0.477493832 \\
            {YAGS} & SPEC 95                        & 44\si{\kilo\byte} & 0.951 & 0.452054795 & 0.463343109 \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[h]
    \centering
    \caption{SPEC benchmarks accuracy result for small predictor sizes.}
    \label{tab:spec-accuracy-small}
    \begin{tabular}{llS[table-format=3.1,table-space-text-post=\si{\kilo\byte}]S[table-format=1.3]S[table-format=1.3]S[table-format=1.3]}
        \toprule
            {\textbf{Predictor}} & {\textbf{Benchmarks}} & {\textbf{Size ($\approx$)}} & {\textbf{Accuracy}} & {\textbf{Accuracy (Normalized)}} & {\textbf{Performance (Normalized)}}\\
        \midrule
            {Combined (local/gshare)} & SPEC 89     & 0.5\si{\kilo\byte} & 0.952 & 0.861878453 & 0.666666667\\
            {Two-level} & N/A                       &                    &       &             &            \\
            {Standard Neural} & SPEC 95 + 00 (int)  & 1.0\si{\kilo\byte} & 0.912 & 0.751381215 & 0.442748092\\
            {Fast-path Neural} & SPEC 95 + 00 (int) & 1.0\si{\kilo\byte} & 0.925 & 0.787292818 & 0.632315522\\
            {gshare} & SPEC 95                      & 0.5\si{\kilo\byte} & 0.690 & 0.138121547 & 0.333333333\\
            {bi-mode} & SPEC 95                     & 0.5\si{\kilo\byte} & 0.730 & 0.248618785 & 0.384223919\\
            {YAGS} & SPEC 95                        & 0.5\si{\kilo\byte} & 0.770 & 0.359116022 & 0.435114504\\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{figure}[H]
    \centering
    \setlength{\figurewidth}{0.7\columnwidth}
    \setlength{\figureheight}{0.45\columnwidth}
    \subimport{resources/}{norm-perf-big-predictors.tikz}
    \caption{Normalized Performance of the Big Predictors}
    \label{fig:performance-norm-big}
\end{figure}

\begin{figure}[H]
    \centering
    \setlength{\figurewidth}{0.7\columnwidth}
    \setlength{\figureheight}{0.45\columnwidth}
    \subimport{resources/}{norm-perf-small-predictors.tikz}
    \caption{Normalized Performance of the Small Predictors}
    \label{fig:performance-norm-small}
\end{figure}

The figures \ref{fig:performance-norm-big} and \ref{fig:performance-norm-small} show the Performance (Normalized) columns for the Tables \ref{tab:spec-accuracy-big} and \ref{tab:spec-accuracy-small} respectively.
The graphs show a much cleared picture than the tables.
Due to the pretty big influence of the size, the difference between the Combined and Two-level result is not that big.
But the absolute difference of \SI{1.1}{\percent} is certainly significant.
As stated earlier, the differences between the different version of SPEC could be the cause of the result disparity between the three data sources.
The results for the bi-mode predictor from the paper by McFarling \cite{hybrid} with the value that is shown here, from the Eden and Mudge \cite{yags} paper, differ approximately \SI{-1}{\percent} in accuracy for the bigger sizes and \SI{20}{\percent} for the smaller sizes.
So the smaller size predictor seems to suffer more from the context switching and the changes from SPEC 89 to SPEC 94.
While the bigger one gains a little, although this might fall within the margin of error.
The same numbers for the gshare predictors are \SI{3.3}{\percent} and \SI{26}{\percent} respectively.

\subsubsection*{Combined}
The combined branch predictor is benchmarked using SPEC 89, running the programs for 10 million instructions.
For large predictor sizes, around 64 KB, table \ref{tab:spec-accuracy-big} shows that the accuracy of the best performing predictor combination appears to be two-level/gshare as it approaches 98.1\%. This is better than either of the two alone. Decreasing the size of the predictor leads to less accuracy as shown in table \ref{tab:spec-accuracy-small}. However the combined predictor still has the best prediction accuracy. A great advantage of the combined predictor is that it's very adaptable. There are a lot of combinations of predictors that could possibly be implemented and if resources are at a premium, the predictor can easily be shrunk at the cost of performance.

\subsubsection*{Two-level}
In nine of the SPEC benchmarks, two-level adaptive training manages to attain an average accuracy of 97\% with a 12 bit 512-entry 4-way associative history register table with also a pattern table comprised of saturating counters. 
The size of the two-level predictor that is mentioned in the table is calculated as the addition of the size of the HRT (512 entries of 12 bits) and the PT ($2^{12}$ entries of 2 bits). 
The data in Yeh and Patt's paper does not provide a clear comparison based on predictor size which is why the two-level entry in the small predictor table is left blank \cite{twolevel}. 
Adjusting the amount of bits in the history registers and the amount of history registers in the HRT still do allow for a predictor that is very adjustable in size.

\subsubsection*{Standard Neural, Fast-path Neural}
The neural predictors are mainly simulated with the SPEC CPU 2000 integer benchmark next to some SPEC 95 integer benchmarks that are not duplicated in SPEC 2000. From table \ref{tab:spec-accuracy-big} it can be seen that the standard neural branch prediction and the fast-path neural branch prediction are not among the best. Looking at table \ref{tab:spec-accuracy-small} however, the predictors are now the second best options. In both cases is the fast-path based algorithm faster than the standard implementation. The parallelization of the algorithm proves to be beneficial. \\
Decreasing the size has little influence on the prediction accuracy with about a difference of 2 percent point. Similarly however this means that increasing the size will not lead to much better accuracy. The fast-path based algorithm takes the path history into account but this influence levels off as the size increases.

\subsubsection*{gshare, bi-mode, YAGS}
gshare, bi-mode, and YAGS predictors were simulated using all 8 SPEC 95 benchmarks interleaved with each 60 000 instructions to simulate a high context switch environment.
This might also be why the accuracy of these algorithms is relatively poor compared to the other paper's figures.

YAGS performs much better when it is scaled down compared to the gshare and bi-mode predictors \cite{yags}.
This is because the YAGS prediction contains less useless information in its tables, as described in subsection \ref{ssec:yags}.
This way one can get the same performance with less resources.

Eden and Mudge believe that they have not shown the full potential of this predictor scheme.
Due to the relatively small size of the direction caches, the history register is also quite small, hence the predictor has reduced correlation information.
A proposed improvement is to add those lost history bits as tags.
This can be done for all the predictors in the paper, but it appears that the overhead would be much smaller for the YAGS prediction scheme \cite{yags}. 
